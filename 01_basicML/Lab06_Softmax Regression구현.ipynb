{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch06 Softmax Regression구현.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMp8sQSOkmmvCDpxU3Ici+W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["이번 챕터에서는 소프트맥스 회귀를 로우-레벨과 F.cross_entropy를 사용해서 구현해보겠습니다."],"metadata":{"id":"cJqG_mzeVW-1"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"9wyFsVUPVR5D","executionInfo":{"status":"ok","timestamp":1644558673094,"user_tz":-540,"elapsed":6765,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","source":["#훈련 데이터와 레이블을 텐서로 선언합니다.\n","#array -> Tensor\n","x_train = [[1, 2, 1, 1],\n","           [2, 1, 3, 2],\n","           [3, 1, 3, 4],\n","           [4, 1, 5, 5],\n","           [1, 7, 5, 5],\n","           [1, 2, 5, 6],\n","           [1, 6, 6, 6],\n","           [1, 7, 7, 7]] #|x_train|= (8, 4). #of samples = 8 . #of features = 4\n","y_train = [2, 2, 2, 1, 1, 1, 0, 0] #|y_train| = (8,) \n","#y_train은 각 샘플에 대한 레이블인데, 여기서는 0, 1, 2의 값을 가지는 것으로 보아 총 3개의 클래스가 존재\n","x_train = torch.FloatTensor(x_train)\n","y_train = torch.LongTensor(y_train)"],"metadata":{"id":"6xjLHG8XVdM1","executionInfo":{"status":"ok","timestamp":1644558791971,"user_tz":-540,"elapsed":334,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### 1. 소프트맥스 회귀 구현(로우-레벨)"],"metadata":{"id":"yPu3jsANV776"}},{"cell_type":"code","source":["#x_train의 크기와 y_train의 크기를 확인\n","print(x_train.shape)\n","print(y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ev9xWKlMVgFv","executionInfo":{"status":"ok","timestamp":1644558842062,"user_tz":-540,"elapsed":429,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"56dc20db-baed-428f-88da-3ba1b6e2d3ff"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 4])\n","torch.Size([8])\n"]}]},{"cell_type":"code","source":["'''\n","x_train의 크기는 8 × 4이며, y_train의 크기는 8 × 1입니다. \n","그런데 최종 사용할 레이블은 y_train에서 원-핫 인코딩을 한 결과이어야 합니다.(low-level이라 F.cross_entropy안쓸경우)\n","클래스의 개수는 3개이므로 y_train에 원-핫 인코딩한 결과는 8 × 3의 개수를 가져야 합니다.\n","'''\n","y_one_hot = torch.zeros(8, 3)\n","y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n","print(y_one_hot.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2cTLgryxWLMA","executionInfo":{"status":"ok","timestamp":1644558916379,"user_tz":-540,"elapsed":534,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"b4bac3b8-bf6c-497c-960c-7879c535afdd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 3])\n"]}]},{"cell_type":"code","source":["'''\n","y_train에서 원-핫 인코딩을 한 결과인 y_one_hot의 크기는 8 × 3입니다.\n","XW + B -> Y_one_hot |X|=(8,4) |B|=|Y_one_hot|=(8,3)이므로\n","즉, W 행렬의 크기는 4 × 3이어야 합니다.\n","W와 b를 선언하고, 옵티마이저로는 경사 하강법을 사용합니다. \n","그리고 학습률은 0.1로 설정합니다.(Tip: lr의 경우, 사용하는 모델의 논문에서 사용한 lr찾아보기)\n","'''\n","# 모델 초기화\n","W = torch.zeros((4, 3), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True) #BroadCasting되어 (8,3)된당... 맞낭...??\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.1)\n"],"metadata":{"id":"buLOqZi_WeCO","executionInfo":{"status":"ok","timestamp":1644559426480,"user_tz":-540,"elapsed":350,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["#F.softmax()와 torch.log()를 사용하여 가설과 비용 함수를 정의하고,\n","#총 1,000번의 에포크를 수행합니다.\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","  #가설\n","  hypothesis = F.softmax(x_train.matmul(W)+ b, dim = 1)\n","\n","  #Cost function\n","  cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n","\n","  #cost로 hypothesis 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  #100번마다 로그 출력\n","  if epoch % 100 == 0:\n","    print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6yD7h-FfWi6B","executionInfo":{"status":"ok","timestamp":1644559579415,"user_tz":-540,"elapsed":679,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"051d2450-f81c-48b2-e130-5f1cf9c8775f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/1000 Cost: 1.098612\n","Epoch  100/1000 Cost: 0.314640\n","Epoch  200/1000 Cost: 0.178421\n","Epoch  300/1000 Cost: 0.122975\n","Epoch  400/1000 Cost: 0.093058\n","Epoch  500/1000 Cost: 0.074548\n","Epoch  600/1000 Cost: 0.062041\n","Epoch  700/1000 Cost: 0.053055\n","Epoch  800/1000 Cost: 0.046302\n","Epoch  900/1000 Cost: 0.041049\n","Epoch 1000/1000 Cost: 0.036850\n"]}]},{"cell_type":"markdown","source":["## 2. 소프트맥스 회귀 구현하기(하이-레벨)"],"metadata":{"id":"kfovmKsQY_mO"}},{"cell_type":"markdown","source":["이제는 F.cross_entropy()를 사용하여 비용 함수를 구현해보겠습니다. **주의할 점은 F.cross_entropy()는 그 자체로 소프트맥스 함수를 포함하고 있으므로 가설에서는 소프트맥스 함수를 사용할 필요가 없습니다.**\n","\n","위와 동일한 x_train과 y_train을 사용합니다."],"metadata":{"id":"SxUk6CTqZCyi"}},{"cell_type":"code","source":["#훈련 데이터와 레이블을 텐서로 선언합니다.\n","#array -> Tensor\n","x_train = [[1, 2, 1, 1],\n","           [2, 1, 3, 2],\n","           [3, 1, 3, 4],\n","           [4, 1, 5, 5],\n","           [1, 7, 5, 5],\n","           [1, 2, 5, 6],\n","           [1, 6, 6, 6],\n","           [1, 7, 7, 7]] #|x_train|= (8, 4). #of samples = 8 . #of features = 4\n","y_train = [2, 2, 2, 1, 1, 1, 0, 0] #|y_train| = (8,) \n","#y_train은 각 샘플에 대한 레이블인데, 여기서는 0, 1, 2의 값을 가지는 것으로 보아 총 3개의 클래스가 존재\n","x_train = torch.FloatTensor(x_train)\n","y_train = torch.LongTensor(y_train)\n","print(x_train.shape)\n","print(y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3WaWAIeZH-2","executionInfo":{"status":"ok","timestamp":1644559661908,"user_tz":-540,"elapsed":7,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"12fe79c7-7950-4b03-d637-df8bf227d859"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 4])\n","torch.Size([8])\n"]}]},{"cell_type":"code","source":["# 모델 초기화\n","W = torch.zeros((4, 3), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","  #Cost 계산\n","  z = x_train.matmul(W) + b #hypotheis\n","  #주의할 점은 F.cross_entropy()는 그 자체로 소프트맥스 함수를 포함하고 있으므로 \n","  #가설에서는 소프트맥스 함수를 사용할 필요가 없습니다.\n","  cost = F.cross_entropy(z, y_train) #원-핫 벡터를 넣을 필요없이 바로 실제값을 인자로 사용\n","  # cost로 H(x) 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  # 100번마다 로그 출력\n","  if epoch % 100 == 0:\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMGjIFmFZLsC","executionInfo":{"status":"ok","timestamp":1644559881654,"user_tz":-540,"elapsed":427,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"732c78b7-9aa7-4277-e342-4b9a5dfc5492"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/1000 Cost: 1.098612\n","Epoch  100/1000 Cost: 0.761050\n","Epoch  200/1000 Cost: 0.689991\n","Epoch  300/1000 Cost: 0.643229\n","Epoch  400/1000 Cost: 0.604117\n","Epoch  500/1000 Cost: 0.568256\n","Epoch  600/1000 Cost: 0.533922\n","Epoch  700/1000 Cost: 0.500291\n","Epoch  800/1000 Cost: 0.466908\n","Epoch  900/1000 Cost: 0.433507\n","Epoch 1000/1000 Cost: 0.399962\n"]}]},{"cell_type":"markdown","source":["## 3. 소프트맥스 회귀 nn.Module로 구현하기"],"metadata":{"id":"6mPt-hcfaGUk"}},{"cell_type":"markdown","source":["이번에는 nn.Module로 소프트맥스 회귀를 구현해봅시다. 선형 회귀에서 구현에 사용했던 **nn.Linear()를 사용**합니다. output_dim이 1이었던 선형 회귀때와 달리 output_dim은 이제 클래스의 개수여야 합니다."],"metadata":{"id":"OIxmPPylaI3e"}},{"cell_type":"code","source":["# 모델을 선언 및 초기화. 4개의 features(특성)을 가지고 3개의 클래스로 분류. input_dim=4, output_dim=3.\n","model = nn.Linear(4, 3)\n","\n","#아래에서 F.cross_entropy()를 사용할 것이므로 따로 소프트맥스 함수를 가설에 정의하지 않습니다.\n","# optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.cross_entropy(prediction, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 20번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item()\n","        ))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1pglvJxQaRwl","executionInfo":{"status":"ok","timestamp":1644559993673,"user_tz":-540,"elapsed":467,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"7f6b618e-54da-4e9a-d374-25405e0f6ae8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/1000 Cost: 1.655176\n","Epoch  100/1000 Cost: 0.707168\n","Epoch  200/1000 Cost: 0.622998\n","Epoch  300/1000 Cost: 0.564230\n","Epoch  400/1000 Cost: 0.512928\n","Epoch  500/1000 Cost: 0.464686\n","Epoch  600/1000 Cost: 0.417831\n","Epoch  700/1000 Cost: 0.371580\n","Epoch  800/1000 Cost: 0.325683\n","Epoch  900/1000 Cost: 0.281097\n","Epoch 1000/1000 Cost: 0.246529\n"]}]},{"cell_type":"markdown","source":["## 4. 소프트맥스 회귀 클래스로 구현하기"],"metadata":{"id":"vI4V8mKoahej"}},{"cell_type":"markdown","source":["이제 소프트맥스 회귀를 nn.Module을 상속받은 클래스로 구현해봅시다."],"metadata":{"id":"Zr-mGDggajxh"}},{"cell_type":"code","source":["class SoftmaxClassifierModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(4, 3) # Outputdim이 3!\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","\n","model = SoftmaxClassifierModel()\n","\n","# optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.cross_entropy(prediction, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 20번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item()\n","        ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2y59gBNaalb8","executionInfo":{"status":"ok","timestamp":1644560551788,"user_tz":-540,"elapsed":352,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"1d908500-e35f-4c53-8759-e2cc9493c90b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/1000 Cost: 1.772288\n","Epoch  100/1000 Cost: 0.689931\n","Epoch  200/1000 Cost: 0.607083\n","Epoch  300/1000 Cost: 0.549071\n","Epoch  400/1000 Cost: 0.498127\n","Epoch  500/1000 Cost: 0.450025\n","Epoch  600/1000 Cost: 0.403168\n","Epoch  700/1000 Cost: 0.356838\n","Epoch  800/1000 Cost: 0.310965\n","Epoch  900/1000 Cost: 0.267557\n","Epoch 1000/1000 Cost: 0.240278\n"]}]}]}