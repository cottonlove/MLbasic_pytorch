{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch_MiniBatch&DataLoad.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMVF27fPxaV5G+ghu1CO1pH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrq70o4tHNUU","executionInfo":{"status":"ok","timestamp":1641385068629,"user_tz":-540,"elapsed":5,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"5531b4d8-873c-48fd-9575-0568bdef5e6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Parameter containing:\n","tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n","tensor([0.2710], requires_grad=True)]\n","Epoch    0/20 Batch 1/3 Cost: 23071.781250\n","Epoch    0/20 Batch 2/3 Cost: 17581.359375\n","Epoch    0/20 Batch 3/3 Cost: 3703.553467\n","Epoch    1/20 Batch 1/3 Cost: 857.131836\n","Epoch    1/20 Batch 2/3 Cost: 194.912628\n","Epoch    1/20 Batch 3/3 Cost: 103.150658\n","Epoch    2/20 Batch 1/3 Cost: 16.460945\n","Epoch    2/20 Batch 2/3 Cost: 10.970690\n","Epoch    2/20 Batch 3/3 Cost: 2.953053\n","Epoch    3/20 Batch 1/3 Cost: 1.246350\n","Epoch    3/20 Batch 2/3 Cost: 0.095024\n","Epoch    3/20 Batch 3/3 Cost: 0.104377\n","Epoch    4/20 Batch 1/3 Cost: 0.678422\n","Epoch    4/20 Batch 2/3 Cost: 0.123370\n","Epoch    4/20 Batch 3/3 Cost: 0.118080\n","Epoch    5/20 Batch 1/3 Cost: 0.094797\n","Epoch    5/20 Batch 2/3 Cost: 0.531496\n","Epoch    5/20 Batch 3/3 Cost: 0.011198\n","Epoch    6/20 Batch 1/3 Cost: 0.219984\n","Epoch    6/20 Batch 2/3 Cost: 0.042412\n","Epoch    6/20 Batch 3/3 Cost: 0.917457\n","Epoch    7/20 Batch 1/3 Cost: 0.213805\n","Epoch    7/20 Batch 2/3 Cost: 0.640249\n","Epoch    7/20 Batch 3/3 Cost: 0.005267\n","Epoch    8/20 Batch 1/3 Cost: 0.454904\n","Epoch    8/20 Batch 2/3 Cost: 0.011374\n","Epoch    8/20 Batch 3/3 Cost: 0.298689\n","Epoch    9/20 Batch 1/3 Cost: 0.560365\n","Epoch    9/20 Batch 2/3 Cost: 0.011228\n","Epoch    9/20 Batch 3/3 Cost: 0.244160\n","Epoch   10/20 Batch 1/3 Cost: 0.613739\n","Epoch   10/20 Batch 2/3 Cost: 0.024911\n","Epoch   10/20 Batch 3/3 Cost: 0.066365\n","Epoch   11/20 Batch 1/3 Cost: 0.116626\n","Epoch   11/20 Batch 2/3 Cost: 0.045004\n","Epoch   11/20 Batch 3/3 Cost: 0.958777\n","Epoch   12/20 Batch 1/3 Cost: 0.234996\n","Epoch   12/20 Batch 2/3 Cost: 0.154496\n","Epoch   12/20 Batch 3/3 Cost: 0.861088\n","Epoch   13/20 Batch 1/3 Cost: 0.398488\n","Epoch   13/20 Batch 2/3 Cost: 0.304441\n","Epoch   13/20 Batch 3/3 Cost: 0.015352\n","Epoch   14/20 Batch 1/3 Cost: 0.050674\n","Epoch   14/20 Batch 2/3 Cost: 0.482778\n","Epoch   14/20 Batch 3/3 Cost: 0.414241\n","Epoch   15/20 Batch 1/3 Cost: 0.513222\n","Epoch   15/20 Batch 2/3 Cost: 0.068332\n","Epoch   15/20 Batch 3/3 Cost: 0.215187\n","Epoch   16/20 Batch 1/3 Cost: 0.069614\n","Epoch   16/20 Batch 2/3 Cost: 0.052825\n","Epoch   16/20 Batch 3/3 Cost: 1.164131\n","Epoch   17/20 Batch 1/3 Cost: 0.034010\n","Epoch   17/20 Batch 2/3 Cost: 0.486735\n","Epoch   17/20 Batch 3/3 Cost: 0.209226\n","Epoch   18/20 Batch 1/3 Cost: 0.024246\n","Epoch   18/20 Batch 2/3 Cost: 0.129105\n","Epoch   18/20 Batch 3/3 Cost: 1.134490\n","Epoch   19/20 Batch 1/3 Cost: 0.490361\n","Epoch   19/20 Batch 2/3 Cost: 0.154791\n","Epoch   19/20 Batch 3/3 Cost: 0.011797\n","Epoch   20/20 Batch 1/3 Cost: 0.408733\n","Epoch   20/20 Batch 2/3 Cost: 0.194442\n","Epoch   20/20 Batch 3/3 Cost: 0.004536\n","훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.0829]], grad_fn=<AddmmBackward0>)\n"]}],"source":["#DataLoad\n","#Using Dataset, DataLoader to perform minibatch 학습,shuffle, 병렬처리\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.utils.data import TensorDataset # 텐서데이터셋\n","from torch.utils.data import DataLoader # 데이터로더\n","\n","torch.manual_seed(1)\n","\n","#Dataset \n","x_train  =  torch.FloatTensor([[73,  80,  75], \n","                               [93,  88,  93], \n","                               [89,  91,  90], \n","                               [96,  98,  100],   \n","                               [73,  66,  70]])  \n","y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n","\n","dataset = TensorDataset(x_train, y_train)\n","dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n","\n","#모델 초기화\n","model = nn.Linear(3,1)\n","#print(list(model.parameters()))\n","#optimizer 설정\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n","\n","#학습\n","nb_epochs = 20\n","for epoch in range(nb_epochs + 1):\n","  for batch_idx, samples in enumerate(dataloader):\n","    # print(batch_idx)\n","    # print(samples)\n","    x_train, y_train = samples\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train)\n","\n","    # cost로 H(x) 계산\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, batch_idx+1, len(dataloader),\n","        cost.item()\n","        ))\n","    \n","# 임의의 입력 [73, 80, 75]를 선언\n","new_var =  torch.FloatTensor([[73, 80, 75]]) \n","# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) \n","print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "]},{"cell_type":"code","source":["#커스텀 데이터셋\n","import torch\n","import torch.nn.functional as F\n","\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","torch.manual_seed(1)\n","\n","#Dataset 상속\n","class CustomDataset(Dataset):\n","  def __init__(self):\n","    self.x_data = [[73, 80, 75],\n","                   [93, 88, 93],\n","                   [89, 91, 90],\n","                   [96, 98, 100],\n","                   [73, 66, 70]] #feature: 3개, sample: 5개\n","    self.y_data = [[152], [185], [180], [196], [142]]    \n","  # 총 데이터의 개수를 return\n","  def __len__(self):\n","    print(\"length\")\n","    return len(self.x_data) \n","  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n","  def __getitem__(self, idx):\n","    print(\"getitem\")\n","    x = torch.FloatTensor(self.x_data[idx])\n","    #print('x is {}'.format(self.x_data[idx]))\n","    y = torch.FloatTensor(self.y_data[idx])        \n","    return x, y \n","\n","#dataset\n","print(\"dataset\")\n","dataset = CustomDataset()\n","print('dataset length is {}'.format(len(dataset))) #5. \n","dataloader = DataLoader(dataset, batch_size = 3, shuffle=True)\n","print(\"loader\")\n","\n","#model, optimizer 선언 및 초기화\n","model = torch.nn.Linear(3,1) #3개의 독립변수 x에서 하나의 y출력\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n","\n","nb_epochs = 20\n","print(\"start learning\")\n","for epoch in range(nb_epochs + 1):\n","  for batch_idx, samples in enumerate(dataloader):\n","    print(\"learning now\")\n","    # print(batch_idx)\n","    # print(samples)\n","    x_train, y_train = samples\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train)\n","    # cost로 H(x) 계산\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","    \n","\n","    # print('Epoch {:4d}/{} Batch {}/{}'.format(\n","    #     epoch, nb_epochs, batch_idx+1, len(dataloader),\n","    #     ))\n","    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n","        epoch, nb_epochs, batch_idx+1, len(dataloader),\n","        cost.item()\n","        ))\n","    \n","# 임의의 입력 [73, 80, 75]를 선언\n","new_var =  torch.FloatTensor([[73, 80, 75]]) \n","# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) \n","print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qE52x1AZJPzG","executionInfo":{"status":"ok","timestamp":1641388169409,"user_tz":-540,"elapsed":405,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"2c7fbb10-3f9f-45b9-9792-3abc5208bab2"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset\n","length\n","dataset length is 5\n","length\n","length\n","loader\n","start learning\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    0/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    0/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    1/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    1/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    2/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    2/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    3/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    3/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    4/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    4/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    5/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    5/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    6/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    6/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    7/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    7/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    8/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    8/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    9/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch    9/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   10/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   10/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   11/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   11/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   12/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   12/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   13/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   13/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   14/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   14/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   15/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   15/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   16/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   16/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   17/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   17/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   18/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   18/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   19/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   19/20 Batch 2/2\n","length\n","getitem\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   20/20 Batch 1/2\n","getitem\n","getitem\n","learning now\n","length\n","Epoch   20/20 Batch 2/2\n"]}]}]}