{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pytorch_tensormanipulation2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP3r+1InTqfAWzSTLw+B6En"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzokhmVWZT6w","executionInfo":{"status":"ok","timestamp":1641177643581,"user_tz":-540,"elapsed":360,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"6edd9578-789c-416f-84a6-989d05a4bcf0"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 3])\n","tensor([[ 0.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.],\n","        [ 9., 10., 11.]])\n","torch.Size([4, 3])\n","tensor([[[ 0.,  1.,  2.]],\n","\n","        [[ 3.,  4.,  5.]],\n","\n","        [[ 6.,  7.,  8.]],\n","\n","        [[ 9., 10., 11.]]])\n","torch.Size([4, 1, 3])\n","tensor([[0.],\n","        [1.],\n","        [2.]])\n","torch.Size([3, 1])\n","tensor([0., 1., 2.])\n","torch.Size([3])\n","torch.Size([3])\n","tensor([[0., 1., 2.]])\n","torch.Size([1, 3])\n","tensor([[0., 1., 2.]])\n","torch.Size([1, 3])\n","tensor([[0.],\n","        [1.],\n","        [2.]])\n","torch.Size([3, 1])\n","tensor([[0.],\n","        [1.],\n","        [2.]])\n","torch.Size([3, 1])\n","tensor([1, 2, 3, 4])\n","tensor([1., 2., 3., 4.])\n","tensor([1, 0, 0, 1], dtype=torch.uint8)\n","tensor([1, 0, 0, 1])\n","tensor([1., 0., 0., 1.])\n"]}],"source":["from typing_extensions import Concatenate\n","#import\n","import numpy as np\n","import torch\n","\n","'''view(), squeeze(), and unsqueeze() adjust their shape and dimension \n","while keeping the number of elements'''\n","\n","#5.View = Reshape(Numpy)\n","'''*** Very important ****\n","Resize(reshape) the tensor while keeping the number of elements'''\n","t = np.array([[[0, 1, 2],\n","               [3, 4, 5]],\n","              [[6, 7, 8],\n","               [9, 10, 11]]])\n","ft = torch.FloatTensor(t)\n","\n","print(ft.shape) #torch.Size([2, 2, 3])\n","\n","#Change from 3D tensor to 2D tensor\n","print(ft.view([-1, 3])) # Change ft tensor to a size of (?, 3)\n","#tensor([[ 0.,  1.,  2.],[ 3.,  4.,  5.],[ 6.,  7.,  8.],[ 9., 10., 11.]])\n","print(ft.view([-1, 3]).shape) #torch.Size([4, 3])\n","\n","#Change the shape while maintaining the 3D dimension \n","print(ft.view([-1, 1, 3])) #tensor([[[ 0.,  1.,  2.]], [[ 3.,  4.,  5.]], [[ 6.,  7.,  8.]],[[ 9., 10., 11.]]])\n","print(ft.view([-1, 1, 3]).shape) #torch.Size([4, 1, 3])\n","\n","#6. Squeeze\n","'''Remove dimension which is 1'''\n","ft = torch.FloatTensor([[0], [1], [2]])\n","print(ft)\n","print(ft.shape) #torch.Size([3, 1]\n","\n","print(ft.squeeze()) #remove 2th dimension which is 1. -> tensor([0., 1., 2.])\n","print(ft.squeeze().shape) #torch.Size([3])\n","\n","#7. Unsqueeze\n","'''특정 위치에 1인 차원을 추가한다'''\n","ft = torch.Tensor([0, 1, 2])\n","print(ft.shape) #(3,) torch.Size([3])\n","#첫번째 차원에 1인 차원을 추가\n","print(ft.unsqueeze(0)) # 인덱스가 0부터 시작하므로 0은 첫번째 차원을 의미\n","#tensor([[0., 1., 2.]])\n","print(ft.unsqueeze(0).shape) #torch.Size([1, 3])\n","#(3,)의 크기를 가졌던 1차원 벡터가 (1, 3)의 2차원 텐서로 변경됨\n","#view로 구현시 다음과 같음: 2차원으로 바꾸고 싶으면서 첫번째 차원은 1이기를 원한다면 view에서 (1, -1)을 인자로 사용\n","\n","print(ft.view(1, -1))\n","print(ft.view(1, -1).shape) #torch.Size([1, 3])\n","\n","print(ft.unsqueeze(1)) #tensor([[0.],[1.],[2.]])\n","print(ft.unsqueeze(1).shape) #torch.Size([3, 1])\n","\n","#인자로 -1추가\n","#-1은 인덱스 상으로 마지막 차원을 의미=마지막 차원에 1인 차원을 추가\n","print(ft.unsqueeze(-1)) #tensor([[0.],[1.],[2.]])\n","print(ft.unsqueeze(-1).shape) #torch.Size([3, 1])\n","\n","#8. Type Casting\n","'''Convert data type'''\n","lt = torch.LongTensor([1, 2, 3, 4])\n","print(lt) #tensor([1, 2, 3, 4])\n","#type casting: convert long -> float\n","print(lt.float()) #tensor([1., 2., 3., 4.])\n","\n","bt = torch.ByteTensor([True, False, False, True])\n","print(bt) #tensor([1, 0, 0, 1], dtype=torch.uint8)\n","#type casting: convert byte -> long/float\n","print(bt.long()) #tensor([1, 0, 0, 1])\n","print(bt.float()) #tensor([1., 0., 0., 1.])\n","\n","#9. Concatenate\n","'''두 텐서를 연결. torch.cat([ ]). 어느 차원을 늘릴 것인지를 인자로 '''\n","'''딥 러닝에서는 주로 모델의 입력 또는 중간 연산에서 두 개의 텐서를 연결하는 경우가 많습니다. \n","두 텐서를 연결해서 입력으로 사용하는 것은 두 가지의 정보를 모두 사용한다는 의미를 가지고 있습니다.'''\n","x = torch.FloatTensor([[1, 2], [3, 4]]) #|x| = (2,2)\n","y = torch.FloatTensor([[5, 6], [7, 8]]) #|y| = (2,2)\n","\n","print(torch.cat([x, y], dim=0)) #(4,2). tensor([[1., 2.], [3., 4.],[5., 6.],[7., 8.]])\n","print(torch.cat([x, y], dim=1)) #(2,4). tensor([[1., 2., 5., 6.],[3., 4., 7., 8.]])\n","\n","#10. Stacking\n","'''연결(concatenate)을 하는 또 다른 방법\n","연결을 하는 것보다 스택킹이 더 편리할 때가 있는데, 이는 스택킹이 많은 연산을 포함하고 있기때문'''\n","x = torch.FloatTensor([1, 4]) #|x|=|y|=|z|=(2,)\n","y = torch.FloatTensor([2, 5])\n","z = torch.FloatTensor([3, 6])\n","print(torch.stack([x, y, z])) #tensor([[1., 4.],[2., 5.],[3., 6.]]) size: (3,2)\n","#스택킹은 사실 많은 연산을 한 번에 축약하고 있음.위 작업은 아래의 코드와 동일\n","print(torch.cat([x.unsqueeze(0), y.unsqueeze(0), z.unsqueeze(0)], dim=0))\n","#x.unsqueeze(0),y.unsqueeze(0),z.unsqueeze(0): (2,) -> (1,2)\n","#cat(~~~, dim = 0): (1,2) -> (3,2)\n","\n","print(torch.stack([x, y, z], dim=1)) #두번째 차원이 증가하도록 쌓으라는 의미\n","#[[1],[4]], [[2],[5]], [[3],[6]] -> tensor([[1., 2., 3.],[4., 5., 6.]])\n","\n","#11. One and Zeros\n","'''0으로 채워진 텐서와 1로 채워진 텐서\n","동일한 크기(shape)지만 0/1으로만 값이 채워진 텐서를 생성'''\n","x = torch.FloatTensor([[0, 1, 2], [2, 1, 0]]) #|x| = (2,3)\n","print(torch.ones_like(x)) #tensor([[1., 1., 1.], [1., 1., 1.]])\n","print(torch.zeros_like(x)) #tensor([[0., 0., 0.],[0., 0., 0.]])\n","\n","#12. In-place Operation (덮어쓰기 연산)\n","'''_붙임. 메모리에 새로 선언하지 않고 기존 tensor에 저장'''\n","x = torch.FloatTensor([[1, 2], [3, 4]]) \n","print(x.mul(2.)) #tensor([[2., 4.],[6., 8.]])\n","print(x) #tensor([[1., 2.], [3., 4.]])\n","\n","#In-Place operation\n","print(x.mul_(2.))#tensor([[2., 4.],[6., 8.]])\n","print(x)#tensor([[2., 4.],[6., 8.]])"]}]}