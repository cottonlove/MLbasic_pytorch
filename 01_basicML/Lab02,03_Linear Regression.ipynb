{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorchBasic_Linear Regression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOXiHi7bV22e6j6A9E8fPxh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-whhhxE7u-D3","executionInfo":{"status":"ok","timestamp":1641361625101,"user_tz":-540,"elapsed":280,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"9cceaefe-ccc6-440b-8d72-aca5f5a23f13"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.],\n","        [2.],\n","        [3.]])\n","torch.Size([3, 1])\n","tensor([0.], requires_grad=True)\n","tensor([0.], requires_grad=True)\n","tensor([[0.],\n","        [0.],\n","        [0.]], grad_fn=<AddBackward0>)\n","tensor(18.6667, grad_fn=<MeanBackward0>)\n"]}],"source":["#파이토치로 선형 회귀(linear regression) 구현\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","#이 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.\n","#torch.manual_seed()를 사용한 프로그램의 결과는 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있습니다. 그 이유는 torch.manual_seed()는 난수 발생 순서와 값을 동일하게 보장해준다는 특징때문입니다.\n","torch.manual_seed(1)\n","\n","#1. 훈련 데이터 변수 선언\n","#보통은 불러옴\n","x_train = torch.FloatTensor([[1],[2],[3]])\n","#x_train = torch.FloatTensor([[1,2,3]])\n","y_train = torch.FloatTensor([[2],[4], [6]])\n","\n","print(x_train)\n","print(x_train.shape)\n","\n","#2. W, b 초기화\n","# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시함.\n","W = torch.zeros(1, requires_grad=True) \n","print(W)\n","b = torch.zeros(1, requires_grad=True)\n","print(b)\n","\n","#3. 가설 세우기\n","hypothesis = x_train * W + b\n","print(hypothesis) \n","\n","#4. 비용 함수 선언\n","cost = torch.mean((hypothesis - y_train) ** 2) \n","print(cost)\n","\n","#5. 경사 하강법 구현\n","#SGD: 경사 하강법의 일종\n","optimizer = optim.SGD([W,b], lr = 0.01)\n","\n","# gradient를 0으로 초기화\n","optimizer.zero_grad() \n","# 비용 함수를 미분하여 gradient 계산\n","cost.backward() \n","# W와 b를 업데이트\n","optimizer.step() \n"]},{"cell_type":"code","source":["#전체 코드\n","\n","#데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","\n","#모델 초기화\n","W = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","#optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.01)\n","\n","nb_epochs = 2000 # 원하는만큼 경사 하강법을 반복\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = x_train * W + b\n","\n","    # cost 계산\n","    cost = torch.mean((hypothesis - y_train) ** 2)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, W.item(), b.item(), cost.item()\n","        ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h6RzrhxuyWax","executionInfo":{"status":"ok","timestamp":1641362023475,"user_tz":-540,"elapsed":703,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"23e39255-066d-402f-ddf7-5f0769439f34"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n","Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n","Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n","Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n","Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n","Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n","Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n","Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n","Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n","Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n","Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n","Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n","Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n","Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n","Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n","Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n","Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n","Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n","Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n","Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n","Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"]}]},{"cell_type":"code","source":["#자동 미분(Autograd)\n","'''경사 하강법 코드를 보고있으면 requires_grad=True, backward() 등이 나옵니다.\n"," 이는 파이토치에서 제공하고 있는 자동 미분(Autograd) 기능을 수행하고 있는 것'''\n","\n","import torch\n","#값이 2인 스칼라 텐서 w 선언\n","# required_grad를 True로 설정합니다. 이는 이 텐서에 대한 기울기를 저장하겠다는 의미 -> w.grad에 w에 대한 미분값이 저장됨\n","w = torch.tensor(2.0, requires_grad=True)\n","y = w**2\n","z = 2*y + 5\n","#backward()를 호출하면 해당 수식의 w에 대한 기울기를 계산\n","z.backward()\n","#w.grad를 출력하면 w가 속한 수식을 w로 미분한 값이 저장된 것을 확인\n","print('수식을 w로 미분한 값 : {}'.format(w.grad))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nwAt9trrDJhy","executionInfo":{"status":"ok","timestamp":1641366409000,"user_tz":-540,"elapsed":343,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"3981a374-3fda-4172-aa72-e9f794d01a4a"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["수식을 w로 미분한 값 : 8.0\n"]}]},{"cell_type":"code","source":["#다중 선형 회귀(Multivariable Linear regression)\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.manual_seed(1)\n","\n","# 훈련 데이터\n","x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n","x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n","x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","# 가중치w와 bias b 선언\n","# x가 3개이므로 w도 3개 선언\n","w1 = torch.zeros(1, requires_grad=True)\n","w2 = torch.zeros(1, requires_grad=True)\n","w3 = torch.zeros(1, requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","\n","# optimizer 설정\n","optimizer = optim.SGD([w1,w2,w3,b], lr = 1e-5)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs+1):\n","  #H(x) 계산\n","  hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n","  #cost 계산\n","  cost = torch.mean((hypothesis - y_train) ** 2)\n","  # cost로 H(x) 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n"," # 100번마다 로그 출력\n","  if epoch % 100 == 0:\n","      print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n","      ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TNdJcwuuLRzQ","executionInfo":{"status":"ok","timestamp":1641368588959,"user_tz":-540,"elapsed":836,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"02554620-cb07-4ea1-acf7-0f6a81d7bd84"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n","Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n","Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n","Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n","Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n","Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n","Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n","Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n","Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n","Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n","Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"]}]},{"cell_type":"code","source":["#x의 갯수가 많아지면 위의 방식은 비효율적 -> 가설을 벡터와 행렬 연산으로 표현\n","#훈련 데이터\n","x_train  =  torch.FloatTensor([[73,  80,  75], \n","                               [93,  88,  93], \n","                               [89,  91,  80], \n","                               [96,  98,  100],   \n","                               [73,  66,  70]])  \n","y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n","\n","# 모델 초기화\n","W = torch.zeros((3, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1e-5)\n","\n","nb_epochs = 20\n","for epoch in range(nb_epochs+1):\n","  #H(x) 계산\n","  # 편향 b는 broadcasting되어서 각 샘플에 더해진다.\n","  hypothesis = x_train.matmul(W) + b\n","\n","  #cost 계산\n","  cost = torch.mean((hypothesis-y_train)**2)\n","\n","  #cost로 H(x) 개선\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","\n","  print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n","      ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBCHWth1MR5F","executionInfo":{"status":"ok","timestamp":1641371134810,"user_tz":-540,"elapsed":289,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"d3feb35a-c3ea-40d4-eb27-7407502b4b28"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n","Epoch    1/20 hypothesis: tensor([66.7178, 80.1701, 76.1025, 86.0194, 61.1565]) Cost: 9537.694336\n","Epoch    2/20 hypothesis: tensor([104.5421, 125.6208, 119.2478, 134.7862,  95.8280]) Cost: 3069.590088\n","Epoch    3/20 hypothesis: tensor([125.9858, 151.3882, 143.7087, 162.4333, 115.4844]) Cost: 990.670288\n","Epoch    4/20 hypothesis: tensor([138.1429, 165.9963, 157.5768, 178.1071, 126.6283]) Cost: 322.481873\n","Epoch    5/20 hypothesis: tensor([145.0350, 174.2780, 165.4395, 186.9928, 132.9461]) Cost: 107.717064\n","Epoch    6/20 hypothesis: tensor([148.9423, 178.9730, 169.8976, 192.0301, 136.5279]) Cost: 38.687496\n","Epoch    7/20 hypothesis: tensor([151.1574, 181.6346, 172.4254, 194.8856, 138.5585]) Cost: 16.499043\n","Epoch    8/20 hypothesis: tensor([152.4131, 183.1435, 173.8590, 196.5043, 139.7097]) Cost: 9.365656\n","Epoch    9/20 hypothesis: tensor([153.1250, 183.9988, 174.6723, 197.4217, 140.3625]) Cost: 7.071114\n","Epoch   10/20 hypothesis: tensor([153.5285, 184.4835, 175.1338, 197.9415, 140.7325]) Cost: 6.331847\n","Epoch   11/20 hypothesis: tensor([153.7572, 184.7582, 175.3958, 198.2360, 140.9424]) Cost: 6.092532\n","Epoch   12/20 hypothesis: tensor([153.8868, 184.9138, 175.5449, 198.4026, 141.0613]) Cost: 6.013817\n","Epoch   13/20 hypothesis: tensor([153.9602, 185.0019, 175.6299, 198.4969, 141.1288]) Cost: 5.986785\n","Epoch   14/20 hypothesis: tensor([154.0017, 185.0517, 175.6785, 198.5500, 141.1671]) Cost: 5.976325\n","Epoch   15/20 hypothesis: tensor([154.0252, 185.0798, 175.7065, 198.5800, 141.1888]) Cost: 5.971208\n","Epoch   16/20 hypothesis: tensor([154.0385, 185.0956, 175.7229, 198.5966, 141.2012]) Cost: 5.967835\n","Epoch   17/20 hypothesis: tensor([154.0459, 185.1045, 175.7326, 198.6059, 141.2082]) Cost: 5.964969\n","Epoch   18/20 hypothesis: tensor([154.0501, 185.1094, 175.7386, 198.6108, 141.2122]) Cost: 5.962291\n","Epoch   19/20 hypothesis: tensor([154.0524, 185.1120, 175.7424, 198.6134, 141.2145]) Cost: 5.959664\n","Epoch   20/20 hypothesis: tensor([154.0536, 185.1134, 175.7451, 198.6145, 141.2158]) Cost: 5.957089\n"]}]},{"cell_type":"code","source":["#nn.Module로 구현하는 선형 회귀\n","#파이토치에서 이미 구현되어져 제공되고 있는 함수들을 불러오는 것\n","#파이토치에서는 선형 회귀 모델이 nn.Linear()라는 함수로, 또 평균 제곱오차가 nn.functional.mse_loss()라는 함수로 구현\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(1)\n","\n","#data 선언\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","\n","# 모델을 선언 및 초기화. 단순 선형 회귀이므로 input_dim=1, output_dim=1.\n","model = nn.Linear(1,1)\n","\n","print(list(model.parameters())) #W,b가 랜덤 초기화되어 있음\n","# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n","\n","# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward() # backward 연산\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))\n","\n","#학습된 모델 이용하여 예측\n","# 임의의 입력 4를 선언\n","new_var =  torch.FloatTensor([[4.0]]) \n","# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) # forward 연산\n","# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n","print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) \n","#학습 후 W,b 출력\n","print(list(model.parameters()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"id5A0v3-R_NA","executionInfo":{"status":"ok","timestamp":1641372039723,"user_tz":-540,"elapsed":801,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"a7278d16-97a8-4195-8986-f1e78b51b13b"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["[Parameter containing:\n","tensor([[0.5153]], requires_grad=True), Parameter containing:\n","tensor([-0.4414], requires_grad=True)]\n","Epoch    0/2000 Cost: 13.103541\n","Epoch  100/2000 Cost: 0.002791\n","Epoch  200/2000 Cost: 0.001724\n","Epoch  300/2000 Cost: 0.001066\n","Epoch  400/2000 Cost: 0.000658\n","Epoch  500/2000 Cost: 0.000407\n","Epoch  600/2000 Cost: 0.000251\n","Epoch  700/2000 Cost: 0.000155\n","Epoch  800/2000 Cost: 0.000096\n","Epoch  900/2000 Cost: 0.000059\n","Epoch 1000/2000 Cost: 0.000037\n","Epoch 1100/2000 Cost: 0.000023\n","Epoch 1200/2000 Cost: 0.000014\n","Epoch 1300/2000 Cost: 0.000009\n","Epoch 1400/2000 Cost: 0.000005\n","Epoch 1500/2000 Cost: 0.000003\n","Epoch 1600/2000 Cost: 0.000002\n","Epoch 1700/2000 Cost: 0.000001\n","Epoch 1800/2000 Cost: 0.000001\n","Epoch 1900/2000 Cost: 0.000000\n","Epoch 2000/2000 Cost: 0.000000\n","훈련 후 입력이 4일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward0>)\n","[Parameter containing:\n","tensor([[1.9994]], requires_grad=True), Parameter containing:\n","tensor([0.0014], requires_grad=True)]\n"]}]},{"cell_type":"code","source":["#다중 선형 회귀 구현\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(1)\n","\n","# 데이터\n","#데이터 선언\n","#3개의 x로부터 하나의 y예측. 5개의 sample, 3개의 feature\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","#모델 선언 및 초기화\n","model = nn.Linear(3,1) #3개의 x로부터 하나의 y예측\n","print(list(model.parameters())) #W,b가 랜덤 초기화되어 있음\n","# optimizer 설정. 경사 하강법 SGD를 사용하고\n","#학습률(learning rate)은 0.00001로 정합니다. 파이썬 코드로는 1e-5로도 표기합니다. 0.01로 하지 않는 이유는 기울기가 발산하기 때문\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n","\n","# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward() # backward 연산\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))\n","\n","#학습된 모델 이용하여 예측\n","# 임의의 입력 [73, 80, 75]를 선언\n","new_var =  torch.FloatTensor([[73, 80, 75]]) \n","# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n","pred_y = model(new_var) \n","print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) \n","\n","#학습 후 3개의 w와 b의 값 출력\n","print(list(model.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EK1fKZp8Zdae","executionInfo":{"status":"ok","timestamp":1641372398783,"user_tz":-540,"elapsed":727,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"20690f73-bace-4348-e448-b96595a29026"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["[Parameter containing:\n","tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n","tensor([0.2710], requires_grad=True)]\n","Epoch    0/2000 Cost: 31667.597656\n","Epoch  100/2000 Cost: 0.225993\n","Epoch  200/2000 Cost: 0.223911\n","Epoch  300/2000 Cost: 0.221941\n","Epoch  400/2000 Cost: 0.220059\n","Epoch  500/2000 Cost: 0.218271\n","Epoch  600/2000 Cost: 0.216575\n","Epoch  700/2000 Cost: 0.214950\n","Epoch  800/2000 Cost: 0.213413\n","Epoch  900/2000 Cost: 0.211952\n","Epoch 1000/2000 Cost: 0.210560\n","Epoch 1100/2000 Cost: 0.209232\n","Epoch 1200/2000 Cost: 0.207967\n","Epoch 1300/2000 Cost: 0.206761\n","Epoch 1400/2000 Cost: 0.205619\n","Epoch 1500/2000 Cost: 0.204522\n","Epoch 1600/2000 Cost: 0.203484\n","Epoch 1700/2000 Cost: 0.202485\n","Epoch 1800/2000 Cost: 0.201542\n","Epoch 1900/2000 Cost: 0.200635\n","Epoch 2000/2000 Cost: 0.199769\n","훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.2305]], grad_fn=<AddmmBackward0>)\n","[Parameter containing:\n","tensor([[0.9778, 0.4539, 0.5768]], requires_grad=True), Parameter containing:\n","tensor([0.2802], requires_grad=True)]\n"]}]},{"cell_type":"code","source":["#단순 선형 회귀 클래스로 구현하기\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(1)\n","\n","# 데이터\n","x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","\n","#class로 구현\n","class LinearRegressionModel(nn.Module): #torch.nn.Module을 상속받는 파이썬 클래스\n","  def __init__(self):\n","    super().__init__() #super() 함수를 부르면 여기서 만든 클래스는 nn.Module 클래스의 속성들을 가지고 초기화 됩니다\n","    self.linear = nn.Linear(1,1)\n","  \n","  def forward(self, x):\n","    return self.linear(x)\n","\n","#모델 선언\n","model = LinearRegressionModel()\n","print(list(model.parameters()))\n","\n","# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n","# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward() # backward 연산\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))\n","#학습된 모델로 예측\n","new_val =  torch.FloatTensor([[4.0]])\n","pred_y = model(new_val) \n","print(\"훈련 후 입력이 4.0일 때의 예측값 :\", pred_y)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FGZgcmGgceov","executionInfo":{"status":"ok","timestamp":1641373346965,"user_tz":-540,"elapsed":686,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"be6d8e3e-bbf4-4ff5-8a69-e9c1ec10e276"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["[Parameter containing:\n","tensor([[0.5153]], requires_grad=True), Parameter containing:\n","tensor([-0.4414], requires_grad=True)]\n","Epoch    0/2000 Cost: 13.103541\n","Epoch  100/2000 Cost: 0.002791\n","Epoch  200/2000 Cost: 0.001724\n","Epoch  300/2000 Cost: 0.001066\n","Epoch  400/2000 Cost: 0.000658\n","Epoch  500/2000 Cost: 0.000407\n","Epoch  600/2000 Cost: 0.000251\n","Epoch  700/2000 Cost: 0.000155\n","Epoch  800/2000 Cost: 0.000096\n","Epoch  900/2000 Cost: 0.000059\n","Epoch 1000/2000 Cost: 0.000037\n","Epoch 1100/2000 Cost: 0.000023\n","Epoch 1200/2000 Cost: 0.000014\n","Epoch 1300/2000 Cost: 0.000009\n","Epoch 1400/2000 Cost: 0.000005\n","Epoch 1500/2000 Cost: 0.000003\n","Epoch 1600/2000 Cost: 0.000002\n","Epoch 1700/2000 Cost: 0.000001\n","Epoch 1800/2000 Cost: 0.000001\n","Epoch 1900/2000 Cost: 0.000000\n","Epoch 2000/2000 Cost: 0.000000\n","훈련 후 입력이 4.0일 때의 예측값 : tensor([[7.9989]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["#다중 선형 회귀 클래스로 구현\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","torch.manual_seed(1)\n","\n","# 데이터\n","x_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70]])\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n","\n","#class로 모델 선언\n","class MultivariateLinearRegressionModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n","\n","    def forward(self, x):\n","        return self.linear(x)\n","#모델 선언 및 초기화\n","model = MultivariateLinearRegressionModel()\n","print(list(model.parameters()))\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n","\n","nb_epochs = 2000\n","for epoch in range(nb_epochs+1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","    # model(x_train)은 model.forward(x_train)와 동일함.\n","\n","    # cost 계산\n","    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n","\n","    # cost로 H(x) 개선하는 부분\n","    # gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","    # 비용 함수를 미분하여 gradient 계산\n","    cost.backward()\n","    # W와 b를 업데이트\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","    # 100번마다 로그 출력\n","      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","          epoch, nb_epochs, cost.item()\n","      ))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NUjtB33tec8C","executionInfo":{"status":"ok","timestamp":1641373448611,"user_tz":-540,"elapsed":726,"user":{"displayName":"박윤정","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07255778251027257626"}},"outputId":"c6c63f33-2f13-423e-a16d-107470799152"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["[Parameter containing:\n","tensor([[ 0.2975, -0.2548, -0.1119]], requires_grad=True), Parameter containing:\n","tensor([0.2710], requires_grad=True)]\n","Epoch    0/2000 Cost: 31667.597656\n","Epoch  100/2000 Cost: 0.225993\n","Epoch  200/2000 Cost: 0.223911\n","Epoch  300/2000 Cost: 0.221941\n","Epoch  400/2000 Cost: 0.220059\n","Epoch  500/2000 Cost: 0.218271\n","Epoch  600/2000 Cost: 0.216575\n","Epoch  700/2000 Cost: 0.214950\n","Epoch  800/2000 Cost: 0.213413\n","Epoch  900/2000 Cost: 0.211952\n","Epoch 1000/2000 Cost: 0.210560\n","Epoch 1100/2000 Cost: 0.209232\n","Epoch 1200/2000 Cost: 0.207967\n","Epoch 1300/2000 Cost: 0.206761\n","Epoch 1400/2000 Cost: 0.205619\n","Epoch 1500/2000 Cost: 0.204522\n","Epoch 1600/2000 Cost: 0.203484\n","Epoch 1700/2000 Cost: 0.202485\n","Epoch 1800/2000 Cost: 0.201542\n","Epoch 1900/2000 Cost: 0.200635\n","Epoch 2000/2000 Cost: 0.199769\n"]}]}]}